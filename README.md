# VietMedGPT-LoRA
Supervised fine-tuning Vietnamese medical QA with LoRA + 4-bit on OpenChat 3.5

# OpenChat ViMedQA SFT ğŸ©ºğŸ¦™

> Supervised fine-tuning (LoRA + 4-bit) mÃ´ hÃ¬nh [OpenChat 3.5](https://huggingface.co/openchat) cho tÃ¡c vá»¥ Há»iâ€“ÄÃ¡p Y khoa tiáº¿ng Viá»‡t.

![license: MIT](https://img.shields.io/badge/license-MIT-blue.svg)
![made with ğŸ¤— Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-%F0%9F%A4%97-ff69b4)

---

## Table of Contents
1. [Giá»›i thiá»‡u](#giá»›i-thiá»‡u)
2. [TÃ­nh nÄƒng](#tÃ­nh-nÄƒng)
3. [Chuáº©n bá»‹ mÃ´i trÆ°á»ng](#chuáº©n-bá»‹-mÃ´i-trÆ°á»ng)


---

## Giá»›i thiá»‡u
Notebook `LLM_Finetune_SFT.ipynb` minh hoáº¡ quy trÃ¬nh:
* DÃ¹ng **OpenChat 3.5**;
* Giáº£m dá»¥ng VRAM vá»›i **4-bit quantization** (BitsAndBytes);
* Ãp dá»¥ng **LoRA** (rank 8, Î± 32) vÃ o cÃ¡c táº§ng Q/V-proj;
* ThÃªm prompt Ã©p â€œChain-of-Thoughtâ€ tiáº¿ng Viá»‡t;
* Fine-tune trÃªn **`hungnm/vietnamese-medical-qa`** (â‰ˆ 5 k cáº·p Qâ€“A);
* Xuáº¥t checkpoint `sft-openchat-medqa/`.

Káº¿t quáº£: mÃ´ hÃ¬nh tráº£ lá»i kÃ¨m cÃ¡c bÆ°á»›c giáº£i thÃ­ch trÆ°á»›c khi Ä‘Æ°a ra cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng

---

## TÃ­nh nÄƒng
| | MÃ´ táº£ |
|---|---|
| **ğŸ”§ LoRA** | Chá»‰ huáº¥n luyá»‡n <1 % tham sá»‘ â†’ nhanh & ráº» |
| **ğŸ§® 4-bit** | Tiáº¿t kiá»‡m VRAM gáº¥p ~2,6Ã— so vá»›i fp16 |
| **ğŸ©º ViMedQA** | Domain chuyÃªn biá»‡t Y khoa tiáº¿ng Viá»‡t |
| **ğŸ—£ï¸ CoT Prompt** | ThÃºc Ä‘áº©y lá»i giáº£i cÃ³ bÆ°á»›c láº­p luáº­n rÃµ rÃ ng |
| **âš¡ Trainer API** | Sá»­ dá»¥ng `transformers.Trainer` â€“ dá»… tÃ¡i láº­p |

---

## Chuáº©n bá»‹ mÃ´i trÆ°á»ng

```bash
git clone https://github.com/<username>/openchat-vimedqa-sft.git
cd openchat-vimedqa-sft

# Python â‰¥ 3.10
python -m venv .venv
source .venv/bin/activate

pip install torch transformers datasets peft bitsandbytes accelerate
